{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-04T11:51:23.698612Z",
     "iopub.status.busy": "2021-01-04T11:51:23.698347Z",
     "iopub.status.idle": "2021-01-04T11:51:27.252198Z",
     "shell.execute_reply": "2021-01-04T11:51:27.251398Z",
     "shell.execute_reply.started": "2021-01-04T11:51:23.698585Z"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SQLContext, DataFrame\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import udf, col, regexp_replace, concat\n",
    "import  pyspark.sql.functions as F\n",
    "import spacy\n",
    "import stanza\n",
    "\n",
    "SparkContext.setSystemProperty(\"spark.sql.crossJoin.enabled\", \"true\")\n",
    "SparkContext.setSystemProperty(\"spark.sql.execution.arrow.maxRecordsPerBatch\", \"2000\")\n",
    "\n",
    "sc = SparkContext.getOrCreate()\n",
    "spark = SQLContext(sc)\n",
    "\n",
    "merged_se_cleaned = 'merged_cleaned_files_se_before_extraction-table' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sc.addPyFile(\"gs://workbench.gleanlabs.net/emr_data/user=johanna/nlp_pipeline_new_remap.zip\")\n",
    "sc.addPyFile(\"gs://workbench.gleanlabs.net/emr_data/user=johanna/nlppipeline_stanza_pandas_udfs.zip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lib.nlppipelinenltk import NLPPipelineNLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlpPipeline = NLPPipelineNLTK()  # initialize the class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import pandas_udf\n",
    "import pyspark.sql.types as T\n",
    "import os \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "           \n",
    "def give_stanza_pos(sentences:pd.Series)-> pd.Series:\n",
    "    os.environ[\"ARROW_PRE_0_15_IPC_FORMAT\"] = \"1\"\n",
    "    #use map or apply on the series instead of that, it will be faster\n",
    "    return pd.Series([sent[1] for sent in nlpPipeline.getKeyPhrases(sentences.tolist())])\n",
    "\n",
    "give_stanza_pos_pandas = pandas_udf(give_stanza_pos, returnType= T.ArrayType(StringType()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lib.sparkmethods import stage1\n",
    "import time\n",
    "import nltk\n",
    "dico = {}\n",
    "times_nltk = []\n",
    "for N in [10000, 50000,100000, 1000000]:\n",
    "    # save file not to have files changing along the function (limit is not deterministic)\n",
    "    spark.read.format('delta').load(\"gs://workbench.gleanlabs.net/emr_data/user=johanna/{}\".format(merged_se_cleaned)).limit(N).coalesce(100).write.mode('overwrite').format('delta').save(\"example{}-table\".format(N))\n",
    "    start = time.time()\n",
    "    \n",
    "    # we load the file and repartition it\n",
    "    df = spark.read.format('delta').load(\"example{}-table\".format(N))\n",
    "    df = df.repartition(200, 'docID')\n",
    "    \n",
    "    # usually this is done inside the getKeyPhrases function + remap stage\n",
    "    df = df.filter((~F.col('content').isNull()) & (F.col('content')!=''))\n",
    "    df2 = df.withColumn('sent_tokenize', F.udf(lambda x: nltk.sent_tokenize(x), T.ArrayType(T.StringType()))(F.col('content')))\n",
    "    df3 = df2.select(df2.docID, df2.content,F.explode(F.col('sent_tokenize')).alias('new_content'))\n",
    "    \n",
    "    # stanza kps using batches of sentences\n",
    "    content = df3.withColumn('kps', give_stanza_pos_pandas(F.col('new_content')))\n",
    "    \n",
    "    #write file\n",
    "    content.write.format('delta').save(\"gs://workbench.gleanlabs.net/emr_data/user=johanna/{}\".format('merged_se_extracted_kps_new_posts_{}_stanza-table'.format(N)))\n",
    "    end = time.time() - start\n",
    "    dico[N] = end\n",
    "    times_nltk.append(end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "nltk = {'sentences': times_nltk}\n",
    "    \n",
    "df_nltk = pd.DataFrame(nltk, index = [10000, 50000,100000, 1000000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_nltk.to_csv('stanza_time_pandas_udfs.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-04T11:51:27.253797Z",
     "iopub.status.busy": "2021-01-04T11:51:27.253591Z",
     "iopub.status.idle": "2021-01-04T11:51:34.540603Z",
     "shell.execute_reply": "2021-01-04T11:51:34.539930Z",
     "shell.execute_reply.started": "2021-01-04T11:51:27.253770Z"
    }
   },
   "outputs": [],
   "source": [
    "content = spark.read.parquet(\"gs://workbench.gleanlabs.net/emr_data/user=johanna/{}\".format('merged_se_extracted_kps_new_posts_{}_stanza-table'.format(1000000)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-04T11:51:34.541979Z",
     "iopub.status.busy": "2021-01-04T11:51:34.541789Z",
     "iopub.status.idle": "2021-01-04T11:51:39.829526Z",
     "shell.execute_reply": "2021-01-04T11:51:39.829038Z",
     "shell.execute_reply.started": "2021-01-04T11:51:34.541958Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4690516"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "content.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-04T11:51:39.830519Z",
     "iopub.status.busy": "2021-01-04T11:51:39.830336Z",
     "iopub.status.idle": "2021-01-04T11:51:41.358006Z",
     "shell.execute_reply": "2021-01-04T11:51:41.357427Z",
     "shell.execute_reply.started": "2021-01-04T11:51:39.830498Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+--------------------+--------------------+\n",
      "|   docID|             content|         new_content|                 kps|\n",
      "+--------+--------------------+--------------------+--------------------+\n",
      "|48072226|Your system can't...|Your system can't...|[system, npm pack...|\n",
      "|48072226|Your system can't...|Try follow this i...|[installation ste...|\n",
      "|48081789|This happens beca...|This happens beca...|[page change, com...|\n",
      "|48081789|This happens beca...|I had the exactly...|       [field, data]|\n",
      "|48081789|This happens beca...|This forces the f...|[field, value, ja...|\n",
      "|48081789|This happens beca...|My case: How I th...|        [case, case]|\n",
      "|48088727|How can I iterate...|How can I iterate...|[xml api response...|\n",
      "|48088727|How can I iterate...|I am getting XML ...|[xml response, ap...|\n",
      "|48088727|How can I iterate...|As I am working o...|[xml response, ti...|\n",
      "|48088727|How can I iterate...|Below is the XML ...|[xml response fil...|\n",
      "|48088727|How can I iterate...|Thanks in advance...|           [advance]|\n",
      "|48088727|How can I iterate...|javascript, jquer...|[javascript, jque...|\n",
      "|48096808|Poisson model and...|Poisson model and...|[poisson model, d...|\n",
      "|48096808|Poisson model and...|I am investigatin...|[count data set, ...|\n",
      "|48096808|Poisson model and...|Is it a problem w...|            [coding]|\n",
      "|48096808|Poisson model and...|I am new to R, so...|           [r, help]|\n",
      "|48096808|Poisson model and...|etc... etc.... r,...|      [r, statistic]|\n",
      "|48099458|Yes, there is an ...|Yes, there is an ...|              [spec]|\n",
      "|48101039|You already figur...|You already figur...|                  []|\n",
      "|48101039|You already figur...|You have a pool o...|[pool, uitablevie...|\n",
      "+--------+--------------------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "content.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark3.7",
   "language": "python",
   "name": "pyspark3.7"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}